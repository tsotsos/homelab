# =============================================================================
# HOMELAB CLUSTER CONFIGURATION
# =============================================================================
# Single source of truth for cluster configuration
# Used by: Terraform, deployment scripts, and label management
#
# TALOS IMAGE EXTENSIONS:
# Custom Talos image includes: iscsi-tools, qemu-guest-agent, util-linux-tools
# Kernel args: mitigations=off, clocksource=tsc, tsc=reliable
# When upgrading Talos, update iso_file_id and talos_installer to include these extensions from Factory

# =============================================================================
# CLUSTER SETTINGS
# =============================================================================
cluster:
  name: "kng-cluster"
  endpoint: "https://10.0.2.100:6443"
  vip: "10.0.2.100"
  cni: "none"              # Using Cilium CNI
  use_static_ips: true     # Static IPs instead of DHCP
  use_talos_vip: true      # Talos built-in VIP (not kube-vip)

# =============================================================================
# NETWORK CONFIGURATION
# =============================================================================
network:
  gateway: "10.0.2.1"
  cidr: "24"
  dns_servers:
    - "10.0.2.1"
    - "10.0.1.1"
  bridge: "vmbr1"
  vlan_id: null

# =============================================================================
# PROXMOX CONFIGURATION
# =============================================================================
proxmox:
  endpoint: "https://10.0.1.21:8006/api2/json"
  nodes: ["s01", "s02", "s03"]
  storage:
    primary: "local-lvm"
    secondary: "zfs-pool"
  iso_file_id: "nfs:iso/talos-1.11.5.iso"
  ssh_username: "terraform"

# =============================================================================
# SOFTWARE VERSIONS
# =============================================================================
versions:
  talos: "v1.11.5"
  kubernetes: "v1.34.1"
  cilium: "1.18.4"
  talos_installer: "factory.talos.dev/installer/d61d93b6df604f5b0e89ebaa8792aa98b098c6fc5c666cbddc5b7904a0f1564d:v1.11.5"

# =============================================================================
# NODE LABELS STRATEGY
# =============================================================================
# Labels are applied AFTER successful bootstrap via label-nodes.sh
# 
# Workload Distribution:
#   - Infrastructure nodes (workers 1-3): Core services (ArgoCD, Ingress, Cert-Manager, etc.)
#   - Storage nodes (workers 4-6): Longhorn storage + application workloads
#   - All workers: General application workloads
#
# Label Hierarchy:
#   - node-role.kubernetes.io/worker: Applied to all workers (protected, set post-bootstrap)
#   - node.kng/workload-infra: Infrastructure services (workers 1-3)
#   - node.kng/workload-storage: Storage services (workers 4-6)
#   - node.kng/workload-apps: Application workloads (all workers)
#   - topology.kubernetes.io/zone: Physical host location (HA distribution)

# =============================================================================
# VM DEFAULTS
# =============================================================================
defaults:
  vm:
    cpu_cores: 4
    memory_mb: 8192
    disk_size: "150G"  # Increased from 50G to 150G for all VMs
  control_plane:
    count: 4
    memory_mb: 8192  # Override if needed
    disk_size: "150G" # Increased from 50G to 150G
    etcd_disk_gb: 20  # Reduced from 100GB - 20GB is plenty for etcd
  worker:
    count_per_node: 2
    memory_mb: 12288  # Override if needed
    disk_size: "150G" # Increased from 50G to 150G
    storage_disk_gb: 1500  # 1.5TB for storage nodes (workers 4, 5, 6)
  talos:
    install_disk: "/dev/vda"  # virtio0 maps to /dev/vda
    sysctls:
      # VM performance optimizations
      vm.swappiness: "1"                    # Minimize swap usage
      net.core.somaxconn: "65535"          # Increase max connections
      net.core.netdev_max_backlog: "30000" # Network performance
      net.ipv4.tcp_fin_timeout: "15"       # Faster TCP cleanup
      net.ipv4.tcp_keepalive_time: "300"   # TCP keepalive optimization
      # Network buffer optimizations
      net.core.rmem_default: "262144"      # Default receive buffer
      net.core.rmem_max: "16777216"        # Max receive buffer
      net.core.wmem_default: "262144"      # Default send buffer
      net.core.wmem_max: "16777216"        # Max send buffer
      # File system optimizations
      fs.inotify.max_user_watches: "1048576"   # File watching capacity
      fs.inotify.max_user_instances: "8192"    # Inotify instances
    kubelet_extra_args:
      # Kubelet optimizations
      feature-gates: "RotateKubeletServerCertificate=true"
      max-pods: "250"
    # Note: Kernel args are now in the Factory ISO image (mitigations=off, clocksource=tsc, tsc=reliable)
    features:
      # Talos features
      rbac: true
      stableHostname: true
      apidCheckExtKeyUsage: true
    disabled_services:
      # Services to disable (will be enabled later when needed)
      - "ext-nut-client"
  network:
    mac_addresses:
      control_plane: []  # Automatic assignment if empty
      worker: []         # Automatic assignment if empty

# =============================================================================
# NODE DEFINITIONS
# =============================================================================
# Labels defined here are applied via label-nodes.sh after bootstrap
# Note: node-role.kubernetes.io/* labels are protected and set post-bootstrap

nodes:
  # -------------------------
  # Control Plane Nodes
  # -------------------------
  "kng-cp-1":
    vm_id: 801
    ip_address: "10.0.2.101"
    mac_address: "02:00:00:00:01:01"
    proxmox_node: "s01"
    role: "controlplane"
    cpu_cores: 4
    labels:
      topology.kubernetes.io/zone: "s01"
    
  "kng-cp-2":
    vm_id: 802
    ip_address: "10.0.2.102"
    mac_address: "02:00:00:00:01:02"
    proxmox_node: "s02"
    role: "controlplane"
    labels:
      topology.kubernetes.io/zone: "s02"
    
  "kng-cp-3":
    vm_id: 803
    ip_address: "10.0.2.103"
    mac_address: "02:00:00:00:01:03"
    proxmox_node: "s03"
    role: "controlplane"
    labels:
      topology.kubernetes.io/zone: "s03"
  
  # -------------------------
  # Worker Nodes - Infrastructure Tier
  # -------------------------  
  "kng-worker-1":
    vm_id: 811
    ip_address: "10.0.2.111"
    mac_address: "02:00:00:00:02:01"
    proxmox_node: "s01"
    role: "worker"
    cpu_cores: 4
    storage_disk_gb: 0
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-infra: "true"
      topology.kubernetes.io/zone: "s01"
    
  "kng-worker-2":
    vm_id: 812
    ip_address: "10.0.2.112"
    mac_address: "02:00:00:00:02:02"
    proxmox_node: "s02"
    role: "worker"
    storage_disk_gb: 0
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-infra: "true"
      topology.kubernetes.io/zone: "s02"
    
  "kng-worker-3":
    vm_id: 813
    ip_address: "10.0.2.113"
    mac_address: "02:00:00:00:02:03"
    proxmox_node: "s03"
    role: "worker"
    storage_disk_gb: 0
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-infra: "true"
      topology.kubernetes.io/zone: "s03"
  
  # -------------------------
  # Worker Nodes - Storage Tier
  # -------------------------
  "kng-worker-4":
    vm_id: 814
    ip_address: "10.0.2.114"
    mac_address: "02:00:00:00:02:04"
    proxmox_node: "s01"
    role: "worker"
    cpu_cores: 4
    storage_disk_gb: 1200
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-storage: "true"
      topology.kubernetes.io/zone: "s01"
    
  "kng-worker-5":
    vm_id: 815
    ip_address: "10.0.2.115"
    mac_address: "02:00:00:00:02:05"
    proxmox_node: "s02"
    role: "worker"
    cpu_cores: 4
    storage_disk_gb: 1200
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-storage: "true"
      topology.kubernetes.io/zone: "s02"
    
  "kng-worker-6":
    vm_id: 816
    ip_address: "10.0.2.116"
    mac_address: "02:00:00:00:02:06"
    proxmox_node: "s03"
    role: "worker"
    cpu_cores: 4
    storage_disk_gb: 1200
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-storage: "true"
      topology.kubernetes.io/zone: "s03"

# External nodes configuration
external_nodes:
  # Example external node configuration
  # "kng-edge-1":
  #   ip_address: "10.0.2.120"
  #   role: "worker"
  #   location: "edge"
  #   arch: "arm64"
  #   resources:
  #     memory_gb: 4
  #     cpu_cores: 4

ssh:
  public_key: "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIE4SrYkfM8Xu/9cFpqIOb8Y4OJ3WyPYJRB1zMOoTPJQN george@Georgioss-MacBook-Pro.local"

# Deployment settings
deployment:
  parallel:
    enabled: true
    max_concurrent_nodes: 3
    wait_between_phases: 30
  timeouts:
    node_boot: 600      # 10 minutes
    api_ready: 600      # 10 minutes
    config_apply: 180   # 3 minutes
    cluster_ready: 900  # 15 minutes
  retries:
    max_attempts: 3
    delay_seconds: 30