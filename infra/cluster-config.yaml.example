# =============================================================================
# HOMELAB CLUSTER CONFIGURATION - EXAMPLE
# =============================================================================
# Copy this file to cluster-config.yaml and customize for your environment
# Single source of truth for cluster configuration
# Used by: Terraform, deployment scripts, and label management
#
# TALOS IMAGE EXTENSIONS:
# Custom Talos image includes: crun, iscsi-tools, nut-client, qemu-guest-agent, util-linux-tools
# When upgrading Talos, update iso_file_id to include these extensions from Factory

# =============================================================================
# CLUSTER SETTINGS
# =============================================================================
cluster:
  name: "my-cluster"
  endpoint: "https://192.168.1.100:6443"  # Replace with your VIP
  vip: "192.168.1.100"                    # Replace with your VIP
  cni: "none"              # Using Cilium CNI
  use_static_ips: true     # Static IPs instead of DHCP
  use_talos_vip: true      # Talos built-in VIP (not kube-vip)

# =============================================================================
# NETWORK CONFIGURATION
# =============================================================================
network:
  gateway: "192.168.1.1"              # Replace with your gateway
  cidr: "24"
  dns_servers:
    - "192.168.1.1"                   # Replace with your DNS servers
    - "1.1.1.1"
  bridge: "vmbr0"                     # Replace with your Proxmox bridge
  vlan_id: null                       # Set if using VLANs

# =============================================================================
# PROXMOX CONFIGURATION
# =============================================================================
proxmox:
  endpoint: "https://192.168.1.10:8006/api2/json"  # Replace with your Proxmox endpoint
  nodes: ["pve1", "pve2", "pve3"]                  # Replace with your Proxmox node names
  storage:
    primary: "local-lvm"              # Replace with your storage
    secondary: "zfs-pool"             # Replace with your storage
  iso_file_id: "local:iso/talos-1.11.5.iso"  # Replace with your ISO location
  ssh_username: "terraform"           # Replace with your SSH username

# =============================================================================
# SOFTWARE VERSIONS
# =============================================================================
versions:
  talos: "v1.11.5"
  kubernetes: "v1.34.1"
  cilium: "1.18.4"
  talos_installer: "factory.talos.dev/installer/d61d93b6df604f5b0e89ebaa8792aa98b098c6fc5c666cbddc5b7904a0f1564d:v1.11.5"

# =============================================================================
# NODE LABELS STRATEGY
# =============================================================================
# Labels are applied AFTER successful bootstrap via label-nodes.sh
# 
# Workload Distribution:
#   - Infrastructure nodes (workers 1-3): Core services (ArgoCD, Ingress, Cert-Manager, etc.)
#   - Storage nodes (workers 4-6): Longhorn storage + application workloads
#   - All workers: General application workloads
#
# Label Hierarchy:
#   - node-role.kubernetes.io/worker: Applied to all workers (protected, set post-bootstrap)
#   - node.kng/workload-infra: Infrastructure services (workers 1-3)
#   - node.kng/workload-storage: Storage services (workers 4-6)
#   - node.kng/workload-apps: Application workloads (all workers)
#   - topology.kubernetes.io/zone: Physical host location (HA distribution)

# =============================================================================
# VM DEFAULTS
# =============================================================================
defaults:
  vm:
    cpu_cores: 4
    memory_mb: 8192
    disk_size: "150G"  # Increased from 50G to 150G for all VMs
  control_plane:
    count: 4
    memory_mb: 8192  # Override if needed
    disk_size: "150G" # Increased from 50G to 150G
    etcd_disk_gb: 20  # Reduced from 100GB - 20GB is plenty for etcd
  worker:
    count_per_node: 2
    memory_mb: 12288  # Override if needed
    disk_size: "150G" # Increased from 50G to 150G
    storage_disk_gb: 1500  # 1.5TB for storage nodes (workers 4, 5, 6)
  talos:
    install_disk: "/dev/vda"  # virtio0 maps to /dev/vda
    sysctls:
      # VM performance optimizations
      vm.swappiness: "1"                    # Minimize swap usage
      net.core.somaxconn: "65535"          # Increase max connections
      net.core.netdev_max_backlog: "30000" # Network performance
      net.ipv4.tcp_fin_timeout: "15"       # Faster TCP cleanup
      net.ipv4.tcp_keepalive_time: "300"   # TCP keepalive optimization
      # Network buffer optimizations
      net.core.rmem_default: "262144"      # Default receive buffer
      net.core.rmem_max: "16777216"        # Max receive buffer
      net.core.wmem_default: "262144"      # Default send buffer
      net.core.wmem_max: "16777216"        # Max send buffer
      # File system optimizations
      fs.inotify.max_user_watches: "1048576"   # File watching capacity
      fs.inotify.max_user_instances: "8192"    # Inotify instances
    kubelet_extra_args:
      # Kubelet optimizations
      feature-gates: "RotateKubeletServerCertificate=true"
      max-pods: "250"
    kernel_args:
      - "mitigations=off"       # Disable CPU vulnerability mitigations for better performance
      - "clocksource=tsc"       # Use Time Stamp Counter for efficient timekeeping
      - "tsc=reliable"          # Trust TSC as stable clock source
    features:
      # Talos features
      rbac: true
      stableHostname: true
      apidCheckExtKeyUsage: true
    disabled_services:
      # Services to disable (will be enabled later when needed)
      - "ext-nut-client"
  network:
    mac_addresses:
      control_plane: []  # Automatic assignment if empty
      worker: []         # Automatic assignment if empty

# =============================================================================
# NODE DEFINITIONS
# =============================================================================
# Labels defined here are applied via label-nodes.sh after bootstrap
# Note: node-role.kubernetes.io/* labels are protected and set post-bootstrap

nodes:
  # -------------------------
  # Control Plane Nodes
  # -------------------------
  "cp-1":
    vm_id: 101
    ip_address: "192.168.1.101"        # Replace with your IP
    mac_address: "AA:BB:CC:DD:01:01"   # Replace with your MAC or leave for auto-assignment
    proxmox_node: "pve1"               # Replace with your Proxmox node
    role: "controlplane"
    cpu_cores: 4
    labels:
      topology.kubernetes.io/zone: "zone1"
    
  "cp-2":
    vm_id: 102
    ip_address: "192.168.1.102"
    mac_address: "AA:BB:CC:DD:01:02"
    proxmox_node: "pve2"
    role: "controlplane"
    labels:
      topology.kubernetes.io/zone: "zone2"
    
  "cp-3":
    vm_id: 103
    ip_address: "192.168.1.103"
    mac_address: "AA:BB:CC:DD:01:03"
    proxmox_node: "pve3"
    role: "controlplane"
    labels:
      topology.kubernetes.io/zone: "zone3"
  
  # -------------------------
  # Worker Nodes - Infrastructure Tier
  # -------------------------  
  "worker-1":
    vm_id: 111
    ip_address: "192.168.1.111"
    mac_address: "AA:BB:CC:DD:02:01"
    proxmox_node: "pve1"
    role: "worker"
    cpu_cores: 4
    storage_disk_gb: 0                 # No additional storage disk
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-infra: "true"
      topology.kubernetes.io/zone: "zone1"
    
  "worker-2":
    vm_id: 112
    ip_address: "192.168.1.112"
    mac_address: "AA:BB:CC:DD:02:02"
    proxmox_node: "pve2"
    role: "worker"
    storage_disk_gb: 0
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-infra: "true"
      topology.kubernetes.io/zone: "zone2"
    
  "worker-3":
    vm_id: 113
    ip_address: "192.168.1.113"
    mac_address: "AA:BB:CC:DD:02:03"
    proxmox_node: "pve3"
    role: "worker"
    storage_disk_gb: 0
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-infra: "true"
      topology.kubernetes.io/zone: "zone3"
  
  # -------------------------
  # Worker Nodes - Storage Tier
  # -------------------------
  "worker-4":
    vm_id: 114
    ip_address: "192.168.1.114"
    mac_address: "AA:BB:CC:DD:02:04"
    proxmox_node: "pve1"
    role: "worker"
    cpu_cores: 4
    storage_disk_gb: 1000             # Additional storage disk in GB
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-storage: "true"
      topology.kubernetes.io/zone: "zone1"
    
  "worker-5":
    vm_id: 115
    ip_address: "192.168.1.115"
    mac_address: "AA:BB:CC:DD:02:05"
    proxmox_node: "pve2"
    role: "worker"
    cpu_cores: 4
    storage_disk_gb: 1000
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-storage: "true"
      topology.kubernetes.io/zone: "zone2"
    
  "worker-6":
    vm_id: 116
    ip_address: "192.168.1.116"
    mac_address: "AA:BB:CC:DD:02:06"
    proxmox_node: "pve3"
    role: "worker"
    cpu_cores: 4
    storage_disk_gb: 1000
    labels:
      node-role.kubernetes.io/worker: "worker"
      node.kng/workload-apps: "true"
      node.kng/workload-storage: "true"
      topology.kubernetes.io/zone: "zone3"

# =============================================================================
# SSH CONFIGURATION
# =============================================================================
ssh:
  public_key: "ssh-ed25519 AAAA... your-ssh-key-here user@hostname"  # Replace with your SSH public key

# =============================================================================
# DEPLOYMENT SETTINGS
# =============================================================================
deployment:
  parallel:
    enabled: true
    max_concurrent_nodes: 3
    wait_between_phases: 30
  timeouts:
    node_boot: 600      # 10 minutes
    api_ready: 600      # 10 minutes
    config_apply: 180   # 3 minutes
    cluster_ready: 900  # 15 minutes
  retries:
    max_attempts: 3
    delay_seconds: 30
